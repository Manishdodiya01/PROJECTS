


import os
import librosa
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder , OneHotEncoder
from keras.models import Sequential
from keras.layers import LSTM , Dense , Dropout , BatchNormalization
from keras.callbacks import EarlyStopping





data_dir = "dataverse_files"





def Extract_Feature(file_path):
    audio_data , sr = librosa.load(file_path)
    mfccs = librosa.feature.mfcc(y=audio_data , sr=sr)
    return mfccs





features = []
labels = []
max_pad_len = 100

for root, dirs, files in os.walk(data_dir):
    for file in files:
        if file.endswith('.wav'):
            file_path = os.path.join(root, file)
            emotion = file.split('_')[2]
            emotion = emotion.split('.')[0]
            
            # Extract features and pad/truncate to a fixed length
            mfccs = Extract_Feature(file_path)
            
            # Pad or truncate the array to ensure fixed length
            if mfccs.shape[1] < max_pad_len:
                padded_mfccs = np.pad(mfccs, ((0, 0), (0, max_pad_len - mfccs.shape[1])), mode='constant')
            else:
                padded_mfccs = mfccs[:, :max_pad_len]
            features.append(padded_mfccs)
            labels.append(emotion)


X = features
y = labels





X = np.array(X)





Label_encoder = LabelEncoder()
y = Label_encoder.fit_transform(y)





from keras.utils import to_categorical

y_one_hot = to_categorical(y)





X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)





model = Sequential([
    LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), dropout=0.2, recurrent_dropout=0.2),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(y_one_hot.shape[1], activation='softmax')
])


model.summary()





model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])


# Define early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])





import matplotlib.pyplot as plt


accuracy = history.history['accuracy']
loss = history.history['loss']
val_accuracy = history.history['val_accuracy']
val_loss = history.history['val_accuracy']


plt.plot(accuracy , color='red' , label='TRAINING-ACCURACY')
plt.plot(val_accuracy , color='blue' , label='TEST-ACCURACY')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()


plt.plot(loss , color='red' , label='TRAINING-ACCURACY')
plt.plot(val_loss , color='blue' , label='TEST-ACCURACY')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()


# Make predictions
predictions = model.predict(X_test)

# Convert predicted probabilities to labels
predicted_labels = np.argmax(predictions, axis=1)

# Convert one-hot encoded labels back to original labels
actual_labels = np.argmax(y_test, axis=1)

# Compare predicted labels with actual labels
correct_predictions = (predicted_labels == actual_labels)

# Calculate accuracy
accuracy = np.mean(correct_predictions)
print("Accuracy:", accuracy)





Error_rate = abs(y_test - predictions) * 100
print('MODEL-ERROR-RATE :',Error_rate.mean())


def extract_feature(file_path):
    audio_data, sr = librosa.load(file_path)
    mfccs = librosa.feature.mfcc(y=audio_data, sr=sr)
    return mfccs

def preprocess_data(features, max_pad_len=100):
    padded_features = []
    for mfccs in features:
        if mfccs.shape[1] < max_pad_len:
            padded_mfccs = np.pad(mfccs, ((0, 0), (0, max_pad_len - mfccs.shape[1])), mode='constant')
        else:
            padded_mfccs = mfccs[:, :max_pad_len]
        padded_features.append(padded_mfccs)
    return np.array(padded_features)


def predict_emotion(audio_file_path, model, Label_encoder, max_pad_len=100):
    # Extract features from the audio file
    mfccs = extract_feature(audio_file_path)
    
    # Preprocess the features
    padded_mfccs = preprocess_data([mfccs], max_pad_len)
    
    # Make predictions using the model
    predictions = model.predict(padded_mfccs)
    
    # Convert the predicted labels to emotions
    predicted_label = np.argmax(predictions)
    predicted_emotion = Label_encoder.inverse_transform([predicted_label])[0]
    
    return predicted_emotion

# Usage:
audio_file_path = "Y2meta.app - Sad music meme (cut) (128 kbps).wav"
predicted_emotion = predict_emotion(audio_file_path, model, Label_encoder)
print("Predicted Emotion:", predicted_emotion)


import librosa
import numpy as np
from tensorflow.keras.models import load_model

# Load the trained model
#model = load_model('emotion_model.h5')  # Replace 'emotion_model.h5' with the filename of your trained model

# Function to extract features from audio file
def extract_feature(file_path):
    audio_data, sr = librosa.load(file_path)
    mfccs = librosa.feature.mfcc(y=audio_data, sr=sr)
    return mfccs

# Function to preprocess features
def preprocess_features(features, max_pad_len=100):
    # Pad or truncate the array to ensure fixed length
    if features.shape[1] < max_pad_len:
        padded_features = np.pad(features, ((0, 0), (0, max_pad_len - features.shape[1])), mode='constant')
    else:
        padded_features = features[:, :max_pad_len]
    return padded_features

# Function to predict emotion
def predict_emotion(file_path):
    # Define emotion labels
    emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'ps', 'sad']
    
    # Extract features
    features = extract_feature(file_path)
    # Preprocess features
    padded_features = preprocess_features(features)
    # Reshape features to match model input shape
    reshaped_features = np.expand_dims(padded_features, axis=0)  # Add batch dimension
    # Perform prediction
    predicted_emotion = model.predict(reshaped_features)
    # Get the index of the maximum probability
    predicted_class_index = np.argmax(predicted_emotion, axis=1)[0]
    # Get the corresponding emotion label
    predicted_emotion_label = emotion_labels[predicted_class_index]
    return predicted_emotion_label


file_path = "dataverse_files/OAF_bean_disgust.wav"
predict_emotion(file_path)


model.save('lstm_model.keras')


import librosa
import numpy as np
from tensorflow.keras.models import load_model

# Load the trained model
model = load_model('lstm_model.keras')  # Replace 'emotion_model.h5' with the filename of your trained model

# Function to extract features from audio file
def extract_feature(file_path):
    audio_data, sr = librosa.load(file_path)
    mfccs = librosa.feature.mfcc(y=audio_data, sr=sr)
    return mfccs

# Function to preprocess features
def preprocess_features(features, max_pad_len=100):
    # Pad or truncate the array to ensure fixed length
    if features.shape[1] < max_pad_len:
        padded_features = np.pad(features, ((0, 0), (0, max_pad_len - features.shape[1])), mode='constant')
    else:
        padded_features = features[:, :max_pad_len]
    return padded_features

# Function to predict emotion
def predict_emotion(file_path):
    # Define emotion labels
    emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'ps', 'sad']
    
    # Extract features
    features = extract_feature(file_path)
    # Preprocess features
    padded_features = preprocess_features(features)
    # Reshape features to match model input shape
    reshaped_features = np.expand_dims(padded_features, axis=0)  # Add batch dimension
    # Perform prediction
    predicted_emotion = model.predict(reshaped_features)
    # Get the index of the maximum probability
    predicted_class_index = np.argmax(predicted_emotion, axis=1)[0]
    # Get the corresponding emotion label
    predicted_emotion_label = emotion_labels[predicted_class_index]
    return predicted_emotion_label

file_path = "dataverse_files/OAF_base_disgust.wav"
predict_emotion(file_path)



