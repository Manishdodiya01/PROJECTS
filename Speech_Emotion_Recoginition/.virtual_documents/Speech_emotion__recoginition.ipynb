import os
import librosa
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder , OneHotEncoder
import tensorflow as tf
from tensorflow.keras import layers, models
from keras.models import Sequential
from keras.layers import LSTM , Dense


data_dir = "dataverse_files"


def extract_features(file_path):
    audio_data, sr = librosa.load(file_path)
    mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=40)
    return mfccs


os.walk(data_dir)


from sklearn.preprocessing import OneHotEncoder

# Prepare data and labels
max_pad_len = 100  # Choose the maximum length for padding

features = []
labels = []

for root, dirs, files in os.walk(data_dir):
    for file in files:
        if file.endswith('.wav'):
            file_path = os.path.join(root, file)
            emotion = file.split('_')[2]  # Assuming file names are structured like '..._emotion.wav'
            # Extract features and pad/truncate to a fixed length
            mfccs = extract_features(file_path)
            # Pad or truncate the array to ensure fixed length
            if mfccs.shape[1] < max_pad_len:
                padded_mfccs = np.pad(mfccs, ((0, 0), (0, max_pad_len - mfccs.shape[1])), mode='constant')
            else:
                padded_mfccs = mfccs[:, :max_pad_len]
            features.append(padded_mfccs)
            labels.append(emotion)

# Convert lists to numpy arrays
X = np.array(features)

# One-hot encode the labels
one_hot_encoder = OneHotEncoder(sparse=False)
y = one_hot_encoder.fit_transform(np.array(labels).reshape(-1, 1))

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


emotion.split('.')[0]


X_train.shape


X.shape


y_train.shape


y.shape


model = Sequential([
    LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])),
    Dense(64, activation='relu'),
    Dense(y.shape[1], activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))


# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)


import matplotlib.pyplot as plt

# Visualize training history
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()





# Make predictions
predictions = model.predict(X_test)





predicted_labels = np.argmax(predictions, axis=1)





# Reshape the predicted_labels array to a 2D array with the correct number of columns
predicted_labels_2d = np.zeros((predicted_labels.size, y.shape[1]))
predicted_labels_2d[np.arange(predicted_labels.size), predicted_labels] = 1





# Calculate accuracy
accuracy = np.mean(predicted_labels_2d == y_test)

# Print accuracy
print("Accuracy:", accuracy)


a
