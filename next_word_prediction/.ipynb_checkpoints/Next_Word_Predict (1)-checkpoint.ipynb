{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BP1Jsof8Tfpi"
   },
   "source": [
    "# Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yYZOFwM98hhX",
    "outputId": "f9d20399-1de1-4419-ac02-caf2d5b09e8d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dodiy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import re\n",
    "import nltk\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM , Dense , Embedding , BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "nltk.download('punkt')\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vNg_7k9T07h"
   },
   "source": [
    "# Reading Text Data into Pandas DataFrame with Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CLphJ19NEGFr",
    "outputId": "db419688-f86d-4f54-e16d-e1ad61c45386",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Project Gutenberg's The Adventures of Sherlock...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This eBook is for the use of anyone anywhere a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>almost no restrictions whatsoever.  You may co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>re-use it under the terms of the Project Guten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>with this eBook or online at www.gutenberg.net</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data\n",
       "0  Project Gutenberg's The Adventures of Sherlock...\n",
       "1  This eBook is for the use of anyone anywhere a...\n",
       "2  almost no restrictions whatsoever.  You may co...\n",
       "3  re-use it under the terms of the Project Guten...\n",
       "4     with this eBook or online at www.gutenberg.net"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attempt to read the file, skipping problematic lines and generating a warning\n",
    "df = pd.read_csv(\"artifacts/1661-0.txt\", sep='\\t',names=['data'] )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5g9duEMLUDxx"
   },
   "source": [
    "# Converting DataFrame to Text String\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0Us7ELawEMPo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = df.to_string(index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ny_Gf7KGXgl"
   },
   "source": [
    "# Fitting Tokenizer on Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7EG478hiGswE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts([data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQF20dDMU6a3"
   },
   "source": [
    "#After fitting the tokenizer on the text data, you can access the word counts and the word index using `tokenizer.word_counts` and `tokenizer.word_index` attributes respectively.\n",
    "- `tokenizer.word_counts` provides a dictionary containing the counts of each word in the text data.\n",
    "- `tokenizer.word_index` provides a dictionary mapping each word to its corresponding index in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rWrrFMKvHl-T",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tokenizer.word_counts\n",
    "#tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96evyE1HVGFw"
   },
   "source": [
    "# Splitting Text Data into Sentences and Printing Each Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pX17xaAeIItu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for sentence in data.split('\\n'):\n",
    "  #print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZmSf07fVLgX"
   },
   "source": [
    "# Converting Sentences to Sequences using Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KBaP_MVkJ2JU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for sentence in data.split('\\n'):\n",
    "  #print(tokenizer.texts_to_sequences([sentence])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-P35AZ-VkQw"
   },
   "source": [
    "# Generating Input Sequences for Sequence Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6GU82Uz4NxtV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "\n",
    "for sentence in data.split('\\n'):\n",
    "  tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0] # tokenizer.texts_to_sequences()` method, which converts the sentence into a sequence of integers.\n",
    "\n",
    "  for i in range(1 , len(tokenized_sentence)):\n",
    "    input_sequences.append(tokenized_sentence[:i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZ8S-GKyYf5Q"
   },
   "source": [
    "# Calculate the maximum length of the input sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WvGXPNksRf3j",
    "outputId": "2f3317d5-d516-4840-ae50-89d696bf0fea",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = max(len(x) for x in input_sequences)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xmo66_Y5Y1Wf"
   },
   "source": [
    "# The code uses the pad_sequences() function from Keras.preprocessing.sequence module to pad the input sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "cUUed0ITR9Zf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "padded_input_sequences = pad_sequences(input_sequences , maxlen=max_len , padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1062BthtSgZH",
    "outputId": "cb66a599-e6a9-4ff8-9c65-8a1246b64e92",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,  145, 4789],\n",
       "       [   0,    0,    0, ...,  145, 4789,    1],\n",
       "       [   0,    0,    0, ..., 4789,    1, 1021],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    3,  360,   83],\n",
       "       [   0,    0,    0, ...,  360,   83,  358],\n",
       "       [   0,    0,    0, ...,   83,  358, 1673]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "gKEkBX6zXlxa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Slice the padded input sequences to create input data (X)\n",
    "X = padded_input_sequences[:, :-1]\n",
    "\n",
    "# Slice the padded input sequences to create target data (Y)\n",
    "Y = padded_input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xgIS1BJiZDAV",
    "outputId": "cca90d97-71ad-449e-b0ea-29389f78548b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-SHAPE : (101619, 19)\n",
      "Y-SHAPE : (101619,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X-SHAPE :\",X.shape)\n",
    "print(\"Y-SHAPE :\",Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThXjYEoBaJqU"
   },
   "source": [
    "# Convert target data Y to one-hot encoded format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GfMOhXBiahg7",
    "outputId": "79eb9a2f-44b8-4095-a1cf-45af0a1921b3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFTER-ONE_HOT_ENCODED-Y : (101619, 8931)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "Y = to_categorical(Y , num_classes=vocabulary_size)\n",
    "\n",
    "print(\"AFTER-ONE_HOT_ENCODED-Y :\",Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_aNVebGbwnO",
    "outputId": "5833aa7b-3327-4756-bc10-826b1762e06d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJ448Y0SqPkh"
   },
   "source": [
    "# Sequential Model Architecture with Embedding, LSTM, and Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "QD8ujomXcP54",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=8931 , output_dim=100 ,input_length=19))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(units=8931 , activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "if4gp77_qTkM"
   },
   "source": [
    "# Compiling the Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "-a1_DO62eeX_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy' , optimizer='adam' , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdYLTiRRqXQC"
   },
   "source": [
    "# Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dxMuCxIZepXO",
    "outputId": "c2ea9e31-680d-448c-d3c9-24df2d74c949",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 19, 100)           893100    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 150)               150600    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8931)              1348581   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2392281 (9.13 MB)\n",
      "Trainable params: 2392281 (9.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuD6bAg0qlST"
   },
   "source": [
    "# Training the Sequential Model with Early Stopping Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "op-cuAJWerVR",
    "outputId": "9f3d77c2-7619-495e-ebc4-be19b7616516"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3176/3176 [==============================] - 118s 37ms/step - loss: 6.2647 - accuracy: 0.0763\n",
      "Epoch 2/100\n",
      "3176/3176 [==============================] - 123s 39ms/step - loss: 5.4942 - accuracy: 0.1274\n",
      "Epoch 3/100\n",
      "3176/3176 [==============================] - 135s 43ms/step - loss: 5.0766 - accuracy: 0.1541\n",
      "Epoch 4/100\n",
      "3176/3176 [==============================] - 127s 40ms/step - loss: 4.7233 - accuracy: 0.1753\n",
      "Epoch 5/100\n",
      "3176/3176 [==============================] - 121s 38ms/step - loss: 4.4016 - accuracy: 0.1941\n",
      "Epoch 6/100\n",
      "3176/3176 [==============================] - 120s 38ms/step - loss: 4.1006 - accuracy: 0.2170\n",
      "Epoch 7/100\n",
      "3176/3176 [==============================] - 139s 44ms/step - loss: 3.8173 - accuracy: 0.2457\n",
      "Epoch 8/100\n",
      "3176/3176 [==============================] - 143s 45ms/step - loss: 3.5511 - accuracy: 0.2776\n",
      "Epoch 9/100\n",
      "3176/3176 [==============================] - 121s 38ms/step - loss: 3.3014 - accuracy: 0.3114\n",
      "Epoch 10/100\n",
      "3176/3176 [==============================] - 119s 38ms/step - loss: 3.0722 - accuracy: 0.3475\n",
      "Epoch 11/100\n",
      "3176/3176 [==============================] - 127s 40ms/step - loss: 2.8595 - accuracy: 0.3849\n",
      "Epoch 12/100\n",
      "3176/3176 [==============================] - 126s 40ms/step - loss: 2.6661 - accuracy: 0.4196\n",
      "Epoch 13/100\n",
      "3176/3176 [==============================] - 127s 40ms/step - loss: 2.4855 - accuracy: 0.4530\n",
      "Epoch 14/100\n",
      "3176/3176 [==============================] - 125s 39ms/step - loss: 2.3223 - accuracy: 0.4844\n",
      "Epoch 15/100\n",
      "3176/3176 [==============================] - 124s 39ms/step - loss: 2.1718 - accuracy: 0.5153\n",
      "Epoch 16/100\n",
      "3176/3176 [==============================] - 120s 38ms/step - loss: 2.0340 - accuracy: 0.5432\n",
      "Epoch 17/100\n",
      "3176/3176 [==============================] - 121s 38ms/step - loss: 1.9091 - accuracy: 0.5702\n",
      "Epoch 18/100\n",
      "3176/3176 [==============================] - 115s 36ms/step - loss: 1.7947 - accuracy: 0.5943\n",
      "Epoch 19/100\n",
      "3176/3176 [==============================] - 115s 36ms/step - loss: 1.6913 - accuracy: 0.6162\n",
      "Epoch 20/100\n",
      "3176/3176 [==============================] - 116s 37ms/step - loss: 1.5932 - accuracy: 0.6364\n",
      "Epoch 21/100\n",
      "3176/3176 [==============================] - 115s 36ms/step - loss: 1.5064 - accuracy: 0.6565\n",
      "Epoch 22/100\n",
      "3176/3176 [==============================] - 116s 36ms/step - loss: 1.4277 - accuracy: 0.6733\n",
      "Epoch 23/100\n",
      "3176/3176 [==============================] - 117s 37ms/step - loss: 1.3557 - accuracy: 0.6897\n",
      "Epoch 24/100\n",
      "3176/3176 [==============================] - 114s 36ms/step - loss: 1.2885 - accuracy: 0.7035\n",
      "Epoch 25/100\n",
      "3176/3176 [==============================] - 114s 36ms/step - loss: 1.2277 - accuracy: 0.7152\n",
      "Epoch 26/100\n",
      "3176/3176 [==============================] - 119s 37ms/step - loss: 1.1705 - accuracy: 0.7304\n",
      "Epoch 27/100\n",
      "3176/3176 [==============================] - 116s 36ms/step - loss: 1.1185 - accuracy: 0.7406\n",
      "Epoch 28/100\n",
      "3176/3176 [==============================] - 116s 36ms/step - loss: 1.0709 - accuracy: 0.7519\n",
      "Epoch 29/100\n",
      "3176/3176 [==============================] - 115s 36ms/step - loss: 1.0262 - accuracy: 0.7609\n",
      "Epoch 30/100\n",
      "3176/3176 [==============================] - 114s 36ms/step - loss: 0.9851 - accuracy: 0.7695\n",
      "Epoch 31/100\n",
      "3176/3176 [==============================] - 114s 36ms/step - loss: 0.9490 - accuracy: 0.7794\n",
      "Epoch 32/100\n",
      "3176/3176 [==============================] - 114s 36ms/step - loss: 0.9144 - accuracy: 0.7854\n",
      "Epoch 33/100\n",
      "3176/3176 [==============================] - 114s 36ms/step - loss: 0.8823 - accuracy: 0.7938\n",
      "Epoch 34/100\n",
      "3176/3176 [==============================] - 114s 36ms/step - loss: 0.8545 - accuracy: 0.7992\n",
      "Epoch 35/100\n",
      "3176/3176 [==============================] - 114s 36ms/step - loss: 0.8282 - accuracy: 0.8040\n",
      "Epoch 36/100\n",
      "3176/3176 [==============================] - 114s 36ms/step - loss: 0.8029 - accuracy: 0.8098\n",
      "Epoch 37/100\n",
      "3176/3176 [==============================] - 116s 37ms/step - loss: 0.7802 - accuracy: 0.8160\n",
      "Epoch 38/100\n",
      "3176/3176 [==============================] - 114s 36ms/step - loss: 0.7584 - accuracy: 0.8199\n",
      "Epoch 39/100\n",
      "3176/3176 [==============================] - 115s 36ms/step - loss: 0.7388 - accuracy: 0.8247\n",
      "Epoch 40/100\n",
      "3176/3176 [==============================] - 113s 36ms/step - loss: 0.7227 - accuracy: 0.8277\n",
      "Epoch 41/100\n",
      "3176/3176 [==============================] - 113s 35ms/step - loss: 0.7059 - accuracy: 0.8314\n",
      "Epoch 42/100\n",
      "3176/3176 [==============================] - 112s 35ms/step - loss: 0.6909 - accuracy: 0.8342\n",
      "Epoch 43/100\n",
      "3176/3176 [==============================] - 115s 36ms/step - loss: 0.6758 - accuracy: 0.8373\n",
      "Epoch 44/100\n",
      "3176/3176 [==============================] - 114s 36ms/step - loss: 0.6628 - accuracy: 0.8401\n",
      "Epoch 45/100\n",
      "3176/3176 [==============================] - 117s 37ms/step - loss: 0.6502 - accuracy: 0.8428\n",
      "Epoch 46/100\n",
      "3176/3176 [==============================] - 118s 37ms/step - loss: 0.6407 - accuracy: 0.8451\n",
      "Epoch 47/100\n",
      "3176/3176 [==============================] - 120s 38ms/step - loss: 0.6301 - accuracy: 0.8466\n",
      "Epoch 48/100\n",
      "3176/3176 [==============================] - 116s 36ms/step - loss: 0.6212 - accuracy: 0.8486\n",
      "Epoch 49/100\n",
      "3176/3176 [==============================] - 115s 36ms/step - loss: 0.6098 - accuracy: 0.8516\n",
      "Epoch 50/100\n",
      "3176/3176 [==============================] - 115s 36ms/step - loss: 0.6022 - accuracy: 0.8518\n",
      "Epoch 51/100\n",
      "3176/3176 [==============================] - 114s 36ms/step - loss: 0.5933 - accuracy: 0.8539\n",
      "Epoch 52/100\n",
      "3176/3176 [==============================] - 115s 36ms/step - loss: 0.5878 - accuracy: 0.8554\n",
      "Epoch 53/100\n",
      "3176/3176 [==============================] - 115s 36ms/step - loss: 0.5808 - accuracy: 0.8561\n",
      "Epoch 54/100\n",
      "3176/3176 [==============================] - 115s 36ms/step - loss: 0.5746 - accuracy: 0.8567\n",
      "Epoch 55/100\n",
      "3176/3176 [==============================] - 114s 36ms/step - loss: 0.5678 - accuracy: 0.8588\n",
      "Epoch 56/100\n",
      "3176/3176 [==============================] - 113s 36ms/step - loss: 0.5652 - accuracy: 0.8591\n",
      "Epoch 57/100\n",
      "3176/3176 [==============================] - 115s 36ms/step - loss: 0.5596 - accuracy: 0.8598\n",
      "Epoch 58/100\n",
      "3176/3176 [==============================] - 116s 36ms/step - loss: 0.5527 - accuracy: 0.8617\n",
      "Epoch 59/100\n",
      "3176/3176 [==============================] - 115s 36ms/step - loss: 0.5506 - accuracy: 0.8615\n",
      "Epoch 60/100\n",
      "3176/3176 [==============================] - 116s 36ms/step - loss: 0.5457 - accuracy: 0.8612\n",
      "Epoch 61/100\n",
      "3176/3176 [==============================] - 116s 36ms/step - loss: 0.5407 - accuracy: 0.8640\n",
      "Epoch 62/100\n",
      "3176/3176 [==============================] - 114s 36ms/step - loss: 0.5364 - accuracy: 0.8641\n",
      "Epoch 63/100\n",
      "3176/3176 [==============================] - 114s 36ms/step - loss: 0.5364 - accuracy: 0.8640\n",
      "Epoch 64/100\n",
      "3176/3176 [==============================] - 114s 36ms/step - loss: 0.5351 - accuracy: 0.8627\n",
      "Epoch 65/100\n",
      "3176/3176 [==============================] - 115s 36ms/step - loss: 0.5307 - accuracy: 0.8633\n",
      "Epoch 66/100\n",
      "3176/3176 [==============================] - 113s 36ms/step - loss: 0.5285 - accuracy: 0.8645\n",
      "Epoch 67/100\n",
      "3176/3176 [==============================] - 114s 36ms/step - loss: 0.5194 - accuracy: 0.8670\n",
      "Epoch 68/100\n",
      "3176/3176 [==============================] - 115s 36ms/step - loss: 0.5211 - accuracy: 0.8656\n",
      "Epoch 69/100\n",
      "3176/3176 [==============================] - 115s 36ms/step - loss: 0.5177 - accuracy: 0.8658\n",
      "Epoch 70/100\n",
      "3176/3176 [==============================] - 112s 35ms/step - loss: 0.5149 - accuracy: 0.8663\n",
      "Epoch 71/100\n",
      "3176/3176 [==============================] - 123s 39ms/step - loss: 0.5124 - accuracy: 0.8678\n",
      "Epoch 72/100\n",
      "3176/3176 [==============================] - 120s 38ms/step - loss: 0.5129 - accuracy: 0.8675\n",
      "Epoch 73/100\n",
      "3176/3176 [==============================] - 112s 35ms/step - loss: 0.5115 - accuracy: 0.8671\n",
      "Epoch 74/100\n",
      "3176/3176 [==============================] - 121s 38ms/step - loss: 0.5086 - accuracy: 0.8674\n",
      "Epoch 75/100\n",
      "3176/3176 [==============================] - 120s 38ms/step - loss: 0.5013 - accuracy: 0.8688\n",
      "Epoch 76/100\n",
      "3176/3176 [==============================] - 117s 37ms/step - loss: 0.5033 - accuracy: 0.8684\n",
      "Epoch 77/100\n",
      "3176/3176 [==============================] - 114s 36ms/step - loss: 0.5037 - accuracy: 0.8677\n",
      "Epoch 78/100\n",
      "3176/3176 [==============================] - 115s 36ms/step - loss: 0.5012 - accuracy: 0.8683\n",
      "Epoch 79/100\n",
      "3176/3176 [==============================] - 115s 36ms/step - loss: 0.4977 - accuracy: 0.8689\n",
      "Epoch 80/100\n",
      "3176/3176 [==============================] - 115s 36ms/step - loss: 0.5038 - accuracy: 0.8675\n",
      "Epoch 81/100\n",
      "3176/3176 [==============================] - 115s 36ms/step - loss: 0.4989 - accuracy: 0.8678\n",
      "Epoch 82/100\n",
      "3176/3176 [==============================] - 115s 36ms/step - loss: 0.4950 - accuracy: 0.8698\n",
      "Epoch 83/100\n",
      "3176/3176 [==============================] - 115s 36ms/step - loss: 0.4920 - accuracy: 0.8703\n",
      "Epoch 84/100\n",
      "3176/3176 [==============================] - 115s 36ms/step - loss: 0.4966 - accuracy: 0.8684\n",
      "Epoch 85/100\n",
      "3176/3176 [==============================] - 115s 36ms/step - loss: 0.4932 - accuracy: 0.8687\n",
      "Epoch 86/100\n",
      "3176/3176 [==============================] - 115s 36ms/step - loss: 0.4929 - accuracy: 0.8681\n",
      "Epoch 87/100\n",
      "3176/3176 [==============================] - 115s 36ms/step - loss: 0.4894 - accuracy: 0.8695\n",
      "Epoch 88/100\n",
      "3176/3176 [==============================] - 116s 36ms/step - loss: 0.4890 - accuracy: 0.8704\n",
      "Epoch 89/100\n",
      "3176/3176 [==============================] - 116s 37ms/step - loss: 0.4905 - accuracy: 0.8690\n",
      "Epoch 90/100\n",
      "3176/3176 [==============================] - 116s 36ms/step - loss: 0.4895 - accuracy: 0.8691\n",
      "Epoch 91/100\n",
      "3176/3176 [==============================] - 115s 36ms/step - loss: 0.4900 - accuracy: 0.8687\n",
      "Epoch 92/100\n",
      "3176/3176 [==============================] - 114s 36ms/step - loss: 0.4838 - accuracy: 0.8700\n",
      "Epoch 93/100\n",
      "3176/3176 [==============================] - 114s 36ms/step - loss: 0.4901 - accuracy: 0.8688\n"
     ]
    }
   ],
   "source": [
    "callback = EarlyStopping(monitor='accuracy' , patience=5 , restore_best_weights=True)\n",
    "history = model.fit(X,Y,epochs=100,batch_size=32, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "AqWv2J2DevBb"
   },
   "outputs": [],
   "source": [
    "model.save('my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ubMIyFkelQMD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12Zb3NtUu6Yk"
   },
   "source": [
    "# Generate next words iteratively based on the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LWvLRjwLpfvK",
    "outputId": "46550b89-2084-4952-c9b7-7b8e4cfeb64e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 528ms/step\n",
      "I tell you that I would give one of the provinces possession\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "I tell you that I would give one of the provinces possession possession\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "I tell you that I would give one of the provinces possession possession spectators\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "I tell you that I would give one of the provinces possession possession spectators inner\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "I tell you that I would give one of the provinces possession possession spectators inner wink\n"
     ]
    }
   ],
   "source": [
    "def generate_next_word(text , model , tokenizer , maxlen=19 , padding='pre' , wait_time=0.2 , num_predictions=5):\n",
    "\n",
    "  for i in range(num_predictions):\n",
    "\n",
    "    token_text = tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "    padded_text = pad_sequences([token_text] , maxlen=maxlen , padding=padding)\n",
    "\n",
    "    predict = np.argmax(model.predict(padded_text))\n",
    "\n",
    "    for word , index in tokenizer.word_index.items():\n",
    "      if index == predict:\n",
    "        text = text + \" \" + word\n",
    "        print(text)\n",
    "    time.sleep(wait_time)\n",
    "\n",
    "input_text = \"I tell you that I would give one of the provinces\"\n",
    "generate_next_word(input_text, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=8931 , output_dim=100 ,input_length=19))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dense(units=8931 , activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer 'embedding_1' expected 1 variables, but received 0 variables during loading. Expected: ['embedding_1/embeddings:0']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m create_model()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the weights into the model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124martifacts/my_model.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py:3531\u001b[0m, in \u001b[0;36mLayer.load_own_variables\u001b[1;34m(self, store)\u001b[0m\n\u001b[0;32m   3529\u001b[0m all_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainable_weights \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_trainable_weights\n\u001b[0;32m   3530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(store\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_vars):\n\u001b[1;32m-> 3531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3532\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_vars)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m variables, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3533\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3534\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(store\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m variables during loading. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3535\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[v\u001b[38;5;241m.\u001b[39mname\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mv\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mall_vars]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3536\u001b[0m     )\n\u001b[0;32m   3537\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_vars):\n\u001b[0;32m   3538\u001b[0m     \u001b[38;5;66;03m# TODO(rchao): check shapes and raise errors.\u001b[39;00m\n\u001b[0;32m   3539\u001b[0m     v\u001b[38;5;241m.\u001b[39massign(store[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mValueError\u001b[0m: Layer 'embedding_1' expected 1 variables, but received 0 variables during loading. Expected: ['embedding_1/embeddings:0']"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "# Load the weights into the model\n",
    "model.load_weights('artifacts/my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "oZ62sZbTuM_8",
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer 'embedding' expected 1 variables, but received 0 variables during loading. Expected: ['embedding/embeddings:0']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m  \u001b[38;5;66;03m# Ensure TensorFlow is imported\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the saved model using TensorFlow's load_model function\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124martifacts/my_model.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:254\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m    250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    251\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following argument(s) are not supported \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    252\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith the native Keras format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m         )\n\u001b[1;32m--> 254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    255\u001b[0m         filepath,\n\u001b[0;32m    256\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    258\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    263\u001b[0m     filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    264\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:281\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    278\u001b[0m             asset_store\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:269\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    267\u001b[0m     asset_store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m _load_state(\n\u001b[0;32m    270\u001b[0m     model,\n\u001b[0;32m    271\u001b[0m     weights_store\u001b[38;5;241m=\u001b[39mweights_store,\n\u001b[0;32m    272\u001b[0m     assets_store\u001b[38;5;241m=\u001b[39masset_store,\n\u001b[0;32m    273\u001b[0m     inner_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    274\u001b[0m     visited_trackables\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mset\u001b[39m(),\n\u001b[0;32m    275\u001b[0m )\n\u001b[0;32m    276\u001b[0m weights_store\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m asset_store:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:466\u001b[0m, in \u001b[0;36m_load_state\u001b[1;34m(trackable, weights_store, assets_store, inner_path, skip_mismatch, visited_trackables)\u001b[0m\n\u001b[0;32m    457\u001b[0m     _load_state(\n\u001b[0;32m    458\u001b[0m         child_obj,\n\u001b[0;32m    459\u001b[0m         weights_store,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    463\u001b[0m         visited_trackables\u001b[38;5;241m=\u001b[39mvisited_trackables,\n\u001b[0;32m    464\u001b[0m     )\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(child_obj, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mset\u001b[39m)):\n\u001b[1;32m--> 466\u001b[0m     _load_container_state(\n\u001b[0;32m    467\u001b[0m         child_obj,\n\u001b[0;32m    468\u001b[0m         weights_store,\n\u001b[0;32m    469\u001b[0m         assets_store,\n\u001b[0;32m    470\u001b[0m         inner_path\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mjoin(inner_path, child_attr),\n\u001b[0;32m    471\u001b[0m         skip_mismatch\u001b[38;5;241m=\u001b[39mskip_mismatch,\n\u001b[0;32m    472\u001b[0m         visited_trackables\u001b[38;5;241m=\u001b[39mvisited_trackables,\n\u001b[0;32m    473\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:534\u001b[0m, in \u001b[0;36m_load_container_state\u001b[1;34m(container, weights_store, assets_store, inner_path, skip_mismatch, visited_trackables)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    533\u001b[0m     used_names[name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 534\u001b[0m _load_state(\n\u001b[0;32m    535\u001b[0m     trackable,\n\u001b[0;32m    536\u001b[0m     weights_store,\n\u001b[0;32m    537\u001b[0m     assets_store,\n\u001b[0;32m    538\u001b[0m     inner_path\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mjoin(inner_path, name),\n\u001b[0;32m    539\u001b[0m     skip_mismatch\u001b[38;5;241m=\u001b[39mskip_mismatch,\n\u001b[0;32m    540\u001b[0m     visited_trackables\u001b[38;5;241m=\u001b[39mvisited_trackables,\n\u001b[0;32m    541\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:435\u001b[0m, in \u001b[0;36m_load_state\u001b[1;34m(trackable, weights_store, assets_store, inner_path, skip_mismatch, visited_trackables)\u001b[0m\n\u001b[0;32m    428\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    429\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load weights in object \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrackable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    430\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping object. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    431\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    432\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    433\u001b[0m             )\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 435\u001b[0m         trackable\u001b[38;5;241m.\u001b[39mload_own_variables(weights_store\u001b[38;5;241m.\u001b[39mget(inner_path))\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trackable, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_assets\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m assets_store:\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_mismatch:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py:3531\u001b[0m, in \u001b[0;36mLayer.load_own_variables\u001b[1;34m(self, store)\u001b[0m\n\u001b[0;32m   3529\u001b[0m all_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainable_weights \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_trainable_weights\n\u001b[0;32m   3530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(store\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_vars):\n\u001b[1;32m-> 3531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3532\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_vars)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m variables, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3533\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3534\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(store\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m variables during loading. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3535\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[v\u001b[38;5;241m.\u001b[39mname\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mv\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mall_vars]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3536\u001b[0m     )\n\u001b[0;32m   3537\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_vars):\n\u001b[0;32m   3538\u001b[0m     \u001b[38;5;66;03m# TODO(rchao): check shapes and raise errors.\u001b[39;00m\n\u001b[0;32m   3539\u001b[0m     v\u001b[38;5;241m.\u001b[39massign(store[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mValueError\u001b[0m: Layer 'embedding' expected 1 variables, but received 0 variables during loading. Expected: ['embedding/embeddings:0']"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  # Ensure TensorFlow is imported\n",
    "\n",
    "# Load the saved model using TensorFlow's load_model function\n",
    "loaded_model = tf.keras.models.load_model('artifacts/my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
