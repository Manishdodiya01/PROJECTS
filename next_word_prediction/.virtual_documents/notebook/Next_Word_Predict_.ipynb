


import pandas as pd
import numpy as np
import tensorflow as tf
import keras
import re
import nltk
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import LSTM , Dense , Embedding , BatchNormalization , GRU , Dropout
from keras.callbacks import EarlyStopping
from nltk.tokenize import word_tokenize
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
nltk.download('punkt')





# Attempt to read the file, skipping problematic lines and generating a warning
df = pd.read_csv("1661-0.txt", sep='\t',names=['data'] )
df.head()





data = df.to_string(index=False)





tokenizer = Tokenizer()
tokenizer.fit_on_texts([data])





#tokenizer.word_counts
#tokenizer.word_index





#for sentence in data.split('\n'):
  #print(sentence)





#for sentence in data.split('\n'):
  #print(tokenizer.texts_to_sequences([sentence])[0])





input_sequences = []

for sentence in data.split('\n'):
  tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0] # tokenizer.texts_to_sequences()` method, which converts the sentence into a sequence of integers.

  for i in range(1 , len(tokenized_sentence)):
    input_sequences.append(tokenized_sentence[:i+1])





max_len = max(len(x) for x in input_sequences)
max_len





padded_input_sequences = pad_sequences(input_sequences , maxlen=max_len , padding='pre')


padded_input_sequences


# Slice the padded input sequences to create input data (X)
X = padded_input_sequences[:, :-1]

# Slice the padded input sequences to create target data (Y)
Y = padded_input_sequences[:, -1]


print("X-SHAPE :",X.shape)
print("Y-SHAPE :",Y.shape)





from keras.utils import to_categorical
vocabulary_size = len(tokenizer.word_index) + 1
Y = to_categorical(Y , num_classes=vocabulary_size)

print("AFTER-ONE_HOT_ENCODED-Y :",Y.shape)


Y





X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)





print('vocabulary-size =',vocabulary_size)
print('Max-Len =',max_len)


model = Sequential()
model.add(Embedding(input_dim=vocabulary_size, output_dim=200, input_length=max_len-1))
model.add(Dropout(0.2))  # Dropout layer with 20% dropout rate
model.add(GRU(200, dropout=0.2, recurrent_dropout=0.2))  # GRU layer with 200 units and dropout
model.add(Dense(vocabulary_size, activation='softmax'))





model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])





model.summary()





history = model.fit(X_train, Y_train, epochs=50, batch_size=64, validation_data=(X_test, Y_test), verbose=1)


model.save("next_word_prediction_model.h5")


import joblib
joblib.dump(tokenizer, "tokenizer.joblib")


import time





def generate_next_word(text , model , tokenizer , maxlen=19 , padding='pre' , wait_time=0.2 , num_predictions=5):

  for i in range(num_predictions):

    token_text = tokenizer.texts_to_sequences([text])[0]

    padded_text = pad_sequences([token_text] , maxlen=maxlen , padding=padding)

    predict = np.argmax(model.predict(padded_text))

    for word , index in tokenizer.word_index.items():
      if index == predict:
        text = text + " " + word
        print(text)
    time.sleep(wait_time)

input_text = "I tell you that I would give one of the provinces"
generate_next_word(input_text, model, tokenizer)


import pickle

# Save the function
with open("generate_next_word.pkl", "wb") as f:
    pickle.dump(generate_next_word, f)



from keras.models import load_model

with open("", "rb") as f:
    tokenizer = joblib.load(f)

# Load modela
model = load_model("next_word_prediction_model.h5")

# Load generate_next_word function
with open("generate_next_word.pkl", "rb") as f:
    generate_next_word = joblib.load(f)

def predict_next_word(input_text):
    token_text = tokenizer.texts_to_sequences([input_text])[0]
    padded_text = pad_sequences([token_text], maxlen=19, padding='pre')
    predicted_probabilities = model.predict(padded_text)[0]
    next_word_index = np.argmax(predicted_probabilities)
    next_word = tokenizer.index_word[next_word_index]
    return next_word

def generate_next_word_text(input_text, num_predictions=5):
    for i in range(num_predictions):
        input_text = predict_next_word(input_text)
        print(input_text)
        time.sleep(0.2)

# Example usage
input_text = "I tell you that I would give one of the provinces"
generate_next_word_text(input_text)


import sklearn
sklearn.__version__


import tensorflow
tensorflow.__version__


import keras
keras.__version__


import joblib
joblib.__version__



