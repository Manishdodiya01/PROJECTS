import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
from sklearn.model_selection import train_test_split , GridSearchCV , cross_val_score , RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder , OneHotEncoder , OrdinalEncoder , StandardScaler , MinMaxScaler
from sklearn.metrics import confusion_matrix , classification_report , ConfusionMatrixDisplay , accuracy_score , recall_score , f1_score
from sklearn.pipeline import Pipeline , make_pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier , VotingClassifier , GradientBoostingClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.naive_bayes import BernoulliNB , GaussianNB


import pandas as pd
df = pd.read_excel('Worksheet in Case Study question 2.xlsx')
df.head()





df['policy_bind_date'] = pd.to_datetime(df['policy_bind_date'])
df['policy_bind_year'] = df['policy_bind_date'].dt.year
df['policy_bind_month'] = df['policy_bind_date'].dt.month
df['policy_bind_day'] = df['policy_bind_date'].dt.day

df['incident_date'] = pd.to_datetime(df['incident_date'])
df['incident_year'] = df['incident_date'].dt.year
df['incident_month'] = df['incident_date'].dt.month
df['incident_day'] = df['incident_date'].dt.day

df.drop('policy_bind_date', axis=1, inplace=True)
df.drop('incident_date', axis=1, inplace=True)


df = df.drop(columns=['insured_zip','incident_location','auto_model'],axis=1)


df['policy_csl'] = df['policy_csl'].apply(lambda x: sum(map(int, x.split('/'))) // 2)
df['policy_csl'] = df['policy_csl'].astype(int)


categorical_columns = df.select_dtypes(exclude='number').drop('fraud_reported', axis=1).columns
numerical_columns = df.select_dtypes(include='number').columns


numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values using mean
    ('scaler', StandardScaler())                # Standardize numerical features
])


categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values using most frequent value
    ('encoder', OneHotEncoder(drop='first',sparse_output=False))              # One-hot encode categorical features
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_columns),
        ('cat', categorical_transformer, categorical_columns)
    ],
    remainder='passthrough' 
)


pipeline = Pipeline(steps=[('preprocessor', preprocessor)])


target_mapping = {
    "Y" : 1,
    "N" : 0
}
df['fraud_reported'] = df['fraud_reported'].map(target_mapping)


X = df.drop('fraud_reported',axis=1)
y = df['fraud_reported']
X_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.22,random_state=42)


X_train_tf = pipeline.fit_transform(X_train)
X_test_tf = pipeline.transform(X_test)

# Get the column names after one-hot encoding
column_names = pipeline.named_steps['preprocessor'].transformers_[1][1].named_steps['encoder'].get_feature_names_out(categorical_columns)

X_train_df = pd.DataFrame(X_train_tf, columns=list(numerical_columns) + list(column_names))
X_test_df = pd.DataFrame(X_test_tf, columns=list(numerical_columns) + list(column_names))

# Ensure the order of columns is consistent
X_train_df = X_train_df[X_test_df.columns]


model_list = {
    "Logistic Regression": LogisticRegression(),
    "Decision Tree": DecisionTreeClassifier(),
    "SVM": SVC(),
    "Random Forest": RandomForestClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "XGBoost": XGBClassifier(),
    "CatBoost": CatBoostClassifier(),
    "Bernoulli Naive Bayes": BernoulliNB(),
    "Gaussian Naive Bayes": GaussianNB()
}


for name , model in model_list.items():

    model = model.fit(X_train_df,y_train)
    predict = model.predict(X_test_df)
    
    print('MODEL-NAME =',name)
    print(f'MODEL {name} ACCURACY =',accuracy_score(y_test,predict))
    print(f'CLASSIFICATION REPORT OF {name}')
    print(classification_report(y_test,predict))

    print('*'*55)


xgb_clf = XGBClassifier()
param_grid = {
    'n_estimators': [100, 200, 300],  # Number of trees
    'max_depth': [3, 4, 5],            # Maximum depth of each tree
    'learning_rate': [0.01, 0.05, 0.1],# Learning rate
    'subsample': [0.8, 0.9, 1.0],      # Subsample ratio of the training instances
    'colsample_bytree': [0.8, 0.9, 1.0],# Subsample ratio of columns when constructing each tree
    'gamma': [0, 1, 5],                # Minimum loss reduction required to make a further partition
    'reg_alpha': [0, 0.1, 0.5],        # L1 regularization term on weights
    'reg_lambda': [0, 0.1, 0.5]         # L2 regularization term on weights
}

random_search = RandomizedSearchCV(xgb_clf, param_distributions=param_grid, n_iter=100, cv=5, scoring='accuracy', random_state=42)


# random_search.fit(X_train_df, y_train)

# best_params = random_search.best_params_
#print("Best Parameters:", best_params)
#best_estimator = random_search.best_estimator_
#test_accuracy = best_estimator.score(X_test_df, y_test)
#print("Test Accuracy:", test_accuracy)


# Best Parameters: {'subsample': 0.9, 'reg_lambda': 0, 'reg_alpha': 0.1, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.01,'gamma': 1, 'colsample_bytree': 1.0}


xgb_clf_best = XGBClassifier(subsample=0.9, reg_lambda=0, reg_alpha=0.1, n_estimators=100,
                             max_depth=5, learning_rate=0.01, gamma=1, colsample_bytree=1.0)

xgb_clf_best.fit(X_train_df, y_train)


test_accuracy_best = xgb_clf_best.score(X_test_df, y_test)
print("Test Accuracy with Best Parameters:", test_accuracy_best)


y_pred = xgb_clf_best.predict(X_test_df)


confusion_matrix(y_test,y_pred)


crosstab_result = pd.crosstab(y_test, y_pred)
# Set row and column names
crosstab_result.index.name = "ACTUAL"
crosstab_result.columns.name = "PREDICTED"


crosstab_result


# TN = 133 , FP = 28  y1 = TP= 46 FN = 133
# FN = 13 , TP = 46   n0 = FP = 28 TN = 13


print(classification_report(y_test,y_pred))
print('XGB-RECALL-SCORE =',recall_score(y_test,y_pred))


import joblib
joblib.dump(xgb_clf_best , "XGB_MODEL.joblib")


joblib.dump(pipeline , "Preprocessor.joblib")


model = joblib.load("XGB_MODEL.joblib")
transformer = joblib.load('Preprocessor.joblib')


data = df.iloc[2].values
data


data = [[134, 29, 687698, 'OH', 200, 2000, 1413.14, 5000000, 'FEMALE',
       'PhD', 'sales', 'board-games', 'own-child', 35100, 0,
       'Multi-vehicle Collision', 'Rear Collision', 'Minor Damage',
       'Police', 'NY', 'Columbus', 7, 3, 'NO', 2, 3, 'NO', 34650, 7700,
       3850, 23100, 'Dodge', 2007,2000, 9, 6, 2015, 2, 22]]

columns = ['months_as_customer', 'age', 'policy_number', 'policy_state','policy_csl','policy_deductable',
           'policy_annual_premium', 'umbrella_limit', 'insured_sex', 'insured_education_level',
           'insured_occupation', 'insured_hobbies', 'insured_relationship', 'capital-gains',
           'capital-loss', 'incident_type', 'collision_type', 'incident_severity', 'authorities_contacted',
           'incident_state', 'incident_city', 'incident_hour_of_the_day', 'number_of_vehicles_involved',
           'property_damage', 'bodily_injuries', 'witnesses', 'police_report_available',
           'total_claim_amount', 'injury_claim', 'property_claim', 'vehicle_claim', 'auto_make',
           'auto_year', 'policy_bind_year', 'policy_bind_month', 'policy_bind_day',
           'incident_year', 'incident_month', 'incident_day']

data_df = pd.DataFrame(data, columns=columns)
data = Preprocessor.transform(data_df)
predict = model.predict(data)[0]
if predict == 1:
    print("PERSON IS FRAUD")
else:
    print('PERSON IS ALIGIBLE FOR LOAN')



